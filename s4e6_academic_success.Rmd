---
title: "s4e6_academic_success_classification"
author: "WangYong"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## The goal 
- predict academic risk of students in higher education.

- evaluation metric: accuracy
0.48108 with sample_submission.csv
0.8178 with glmnet and recipe baseline
0.8197 with as_factor most of factor like columns

- machine learning workflow: R tidymodel workflow/workflows



## librar y & load_data

### library

```{r}
library(tidyverse)
library(tidymodels)
# Explicitly resolve conflicts
tidymodels_prefer()
library(finetune)
library(future)
library(purrr)
library(furrr)
library(textrecipes)
library(themis)


library(bonsai)
library(lightgbm)
library(xgboost)
library(ranger)

library(readr)
library(janitor)
library(lubridate)

library(text2vec) # step_dumm_hash to reduce the high dimension impact lead by step_dummy
library(vcd) # visualize categorical data , especial mosaic plot
library(ggmosaic)
library(patchwork) # it can add multi ggplot output together by + /
```

### loading data

```{r}
data_path <- '../input/playground-series-s4e6/'
train<- 
  readr::read_csv(file.path(data_path, 'train.csv'),
                  show_col_types = FALSE)|>
  janitor::clean_names()|>
  mutate(target=as.factor(target))
test <- 
  readr::read_csv(file.path(data_path, 'test.csv'),
                  show_col_types = FALSE)|>
   janitor::clean_names()
submission <-  readr::read_csv(file.path(data_path, 'sample_submission.csv'),show_col_types = FALSE)
```

### quick skim

```{r cache = TRUE}
my_skim <- skimr::skim_with(numeric = skimr::sfl( p25 = NULL, p75 = NULL))
train|> my_skim()
```

```{r cache=TRUE}
test|> skimr::skim()
```

```{r cache=TRUE}
submission |> skimr::skim()
```

### check if train & test is same distribution

```{r}
get_df_var<-function(df){
  df|>
    select(-any_of(c('id','target')))|>
    summarize_all(var)|>
    pivot_longer(cols=everything(),
                 names_to='feature',
                 values_to='variance')

}
list(train=train, test=test)|>
  map_dfr(\(x) get_df_var(x), .id = "dataset") |>
  pivot_wider(names_from=dataset, values_from = variance)|>
  mutate(pct_change=(train-test)/train)#|>arrange(desc(abs(diff)))
```

### Finding of different distribution
- education special need -0.106%
- international 0.126%
- curricular_unit_2nd_sem_credited 0.11%


## EDA
```{r}
train|>names()
```
### chisq_test for all features
```{r}
fct_cols_1 <- c('marital_status','application_mode','application_order','course','daytime_evening_attendance',
              'previous_qualification','previous_qualification_grade','nacionality','mothers_qualification',
              'fathers_qualification', 'mothers_occupation', 'fathers_occupation', 
              'admission_grade', 'displaced', 'educational_special_needs', 
              'debtor', 'tuition_fees_up_to_date', 'gender', 
              'scholarship_holder', 'age_at_enrollment', 'international')
fct_cols_2 <- c("curricular_units_1st_sem_credited", "curricular_units_1st_sem_enrolled",
              "curricular_units_1st_sem_evaluations", "curricular_units_1st_sem_approved",
              "curricular_units_1st_sem_grade", "curricular_units_1st_sem_without_evaluations",
              "curricular_units_2nd_sem_credited", "curricular_units_2nd_sem_enrolled", 
              "curricular_units_2nd_sem_evaluations", "curricular_units_2nd_sem_approved",
              "curricular_units_2nd_sem_grade", "curricular_units_2nd_sem_without_evaluations",
              )
train|>
  select(all_of(c(fct_cols_1, 'target')))|>
  mutate(across(all_of(fct_cols_1),as_factor))|>
  map(~ stats::chisq.test(.x, train$target)) %>%  # 对每一列进行卡方检验
  map_dfr(tidy, .id = "variable")  # 将结果整理为数据框
  select(-method)
```
Some Columns need further study
- educational_speical_needs, chisq statistic value 0.233, pvalue 0.89 (not very useful)
- nacionality , chisq statistic value 86, pvalue -6 (done)
- international, statistic value 0.64, pvalue 0.72
- previous_qualification_grade, parameter is 218
- admission_grad, parameter is 1134
- age_at_enrollment, 90
- fathers_occupation,fathers_qualification, 110
- mothers_occupation,mothers_qualification, 78
- curricular_units_1st_sem_grade, parameter df is 2352
- curricular_units_2nd_sem_grade, parameter df is 2373 


### educational_speical_needs, chisq statistic value 0.233, pvalue 0.89 (not significant, can be removed in future)
```{r, fig.height=3, fig.width=6 }
## fig.height=240
## fig.width=320

train|>mutate(x=as_factor(educational_special_needs)) |> chisq_test(target~x)

train|>janitor::tabyl(target,educational_special_needs ) |>adorn_percentages("row") |>adorn_pct_formatting(digits = 0)|>adorn_ns()
train|>
   ggplot() + 
   geom_mosaic(aes(x=product(target, educational_special_needs ),fill=target)) 
```

### nacionality , chisq statistic value 86, pvalue 1.617041e-06	 (not big, but pvalue is significant , keep)
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

train|>mutate(x=as_factor(nacionality)) |> chisq_test(target~x)

train|>janitor::tabyl(target,nacionality ) |>adorn_percentages("row") |>adorn_pct_formatting(digits = 0)|>adorn_ns()

train|>
   ggplot() + 
   geom_mosaic(aes(x=product(target, nacionality ),fill=target)) 
```
### international, statistic value 0.64, pvalue 0.72 ( remove )
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

train|>mutate(x=as_factor(international)) |> chisq_test(target~x)

train|>janitor::tabyl(target,international ) |>adorn_percentages("row") |>adorn_pct_formatting(digits = 0)|>adorn_ns()
train|>
   ggplot() + 
   geom_mosaic(aes(x=product(target, international ),fill=target)) 
```
### previous_qualification_grade, parameter is 218 (leave it as continuous)
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

anova_result <- 
  train|>
  specify(previous_qualification_grade~target) |>
  hypothesise(null='independence') |>
  calculate(stat='F')

chisq_result <-
  train|>
  mutate(previous_qualification_grade= as_factor(previous_qualification_grade))|>
  specify(previous_qualification_grade~target) |>
  hypothesise(null='independence') |>
  calculate(stat='chisq')

tibble(metric_name = c('anonva','chisquare'),
     metric_value= c(anova_result[[1]], chisq_result[[1]])) 

(train|>ggplot(aes(x=previous_qualification_grade,fill=target)) +geom_density(alpha=0.3) ) /(
train|>ggplot(aes(x=previous_qualification_grade,fill=target)) + geom_boxplot())
```

### admission_grad, parameter is 1134  (leave it as continuous)
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

anova_result <- 
  train|>
  specify(admission_grade~target) |>
  hypothesise(null='independence') |>
  calculate(stat='F')

chisq_result <-
  train|>
  mutate(admission_grade= as_factor(admission_grade))|>
  specify(admission_grade~target) |>
  hypothesise(null='independence') |>
  calculate(stat='chisq')

tibble(metric_name = c('anonva','chisquare'),
     metric_value= c(anova_result[[1]], chisq_result[[1]])) 

(train|>ggplot(aes(x=admission_grade,fill=target)) +geom_density(alpha=0.3) ) /(
train|>ggplot(aes(x=admission_grade,fill=target)) + geom_boxplot())
```



### age_at_enrollment, 90 (leave it as continous)
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

anova_result <- 
  train|>
  specify(age_at_enrollment~target) |>
  hypothesise(null='independence') |>
  calculate(stat='F')

chisq_result <-
  train|>
  mutate(age_at_enrollment= as_factor(age_at_enrollment))|>
  specify(age_at_enrollment~target) |>
  hypothesise(null='independence') |>
  calculate(stat='chisq')

tibble(metric_name = c('anonva','chisquare'),
     metric_value= c(anova_result[[1]], chisq_result[[1]])) 

(train|>ggplot(aes(x=age_at_enrollment,fill=target)) +geom_density(alpha=0.3) ) /(
train|>ggplot(aes(x=age_at_enrollment,fill=target)) + geom_boxplot())
```

### fathers_occupation  110 (TBD)
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

anova_result <- 
  train|>
  specify(fathers_occupation~target) |>
  hypothesise(null='independence') |>
  calculate(stat='F')

chisq_result <-
  train|>
  mutate(fathers_occupation= as_factor(fathers_occupation))|>
  specify(fathers_occupation~target) |>
  hypothesise(null='independence') |>
  calculate(stat='chisq')

tibble(metric_name = c('anonva','chisquare'),
     metric_value= c(anova_result[[1]], chisq_result[[1]])) 

(train|>ggplot(aes(x=fathers_occupation,fill=target)) +geom_density(alpha=0.3) ) /(
train|>ggplot(aes(x=fathers_occupation,fill=target)) + geom_boxplot())
```
### fathers_qualification, 110 (TBD ,drop out centre different)
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

anova_result <- 
  train|>
  specify(fathers_qualification~target) |>
  hypothesise(null='independence') |>
  calculate(stat='F')

chisq_result <-
  train|>
  mutate(fathers_qualification= as_factor(fathers_qualification))|>
  specify(fathers_qualification~target) |>
  hypothesise(null='independence') |>
  calculate(stat='chisq')

tibble(metric_name = c('anonva','chisquare'),
     metric_value= c(anova_result[[1]], chisq_result[[1]])) 

(train|>ggplot(aes(x=fathers_qualification,fill=target)) +geom_density(alpha=0.3) ) /(
train|>ggplot(aes(x=fathers_qualification,fill=target)) + geom_boxplot())
```



### mothers_occupation,mothers_qualification, 78(TBD)
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

anova_result <- 
  train|>
  specify(mothers_occupation~target) |>
  hypothesise(null='independence') |>
  calculate(stat='F')

chisq_result <-
  train|>
  mutate(mothers_occupation= as_factor(mothers_occupation))|>
  specify(mothers_occupation~target) |>
  hypothesise(null='independence') |>
  calculate(stat='chisq')

tibble(metric_name = c('anonva','chisquare'),
     metric_value= c(anova_result[[1]], chisq_result[[1]])) 

(train|>ggplot(aes(x=mothers_occupation,fill=target)) +geom_density(alpha=0.3) ) /(
train|>ggplot(aes(x=mothers_occupation,fill=target)) + geom_boxplot())
```

### mothers_qualification, 78(TBD)
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

anova_result <- 
  train|>
  specify(mothers_qualification~target) |>
  hypothesise(null='independence') |>
  calculate(stat='F')

chisq_result <-
  train|>
  mutate(mothers_qualification= as_factor(mothers_qualification))|>
  specify(mothers_qualification~target) |>
  hypothesise(null='independence') |>
  calculate(stat='chisq')

tibble(metric_name = c('anonva','chisquare'),
     metric_value= c(anova_result[[1]], chisq_result[[1]])) 

(train|>ggplot(aes(x=mothers_qualification,fill=target)) +geom_density(alpha=0.3) ) /(
train|>ggplot(aes(x=mothers_qualification,fill=target)) + geom_boxplot())
```


### curricular_units_1st_sem_grade, parameter df is 2352 (important continuous variable)
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

anova_result <- 
  train|>
  specify(curricular_units_1st_sem_grade~target) |>
  hypothesise(null='independence') |>
  calculate(stat='F')

chisq_result <-
  train|>
  mutate(curricular_units_1st_sem_grade= as_factor(curricular_units_1st_sem_grade))|>
  specify(curricular_units_1st_sem_grade~target) |>
  hypothesise(null='independence') |>
  calculate(stat='chisq')

tibble(metric_name = c('anonva','chisquare'),
     metric_value= c(anova_result[[1]], chisq_result[[1]])) 


(train|>ggplot(aes(x=curricular_units_1st_sem_grade,fill=target)) +geom_density(alpha=0.3) ) /(
train|>ggplot(aes(x=curricular_units_1st_sem_grade,fill=target)) + geom_boxplot())
```
### curricular_units_2nd_sem_grade, parameter df is 2373 
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

anova_result <- 
  train|>
  specify(curricular_units_2nd_sem_grade~target) |>
  hypothesise(null='independence') |>
  calculate(stat='F')

chisq_result <-
  train|>
  mutate(curricular_units_2nd_sem_grade= as_factor(curricular_units_2nd_sem_grade))|>
  specify(curricular_units_2nd_sem_grade~target) |>
  hypothesise(null='independence') |>
  calculate(stat='chisq')

tibble(metric_name = c('anonva','chisquare'),
     metric_value= c(anova_result[[1]], chisq_result[[1]])) 

(train|>ggplot(aes(x=curricular_units_2nd_sem_grade,fill=target)) +geom_density(alpha=0.3) ) /(
train|>ggplot(aes(x=curricular_units_2nd_sem_grade,fill=target)) + geom_boxplot())
```
### unemployment_rate
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

anova_result <- 
  train|>
  specify(unemployment_rate~target) |>
  hypothesise(null='independence') |>
  calculate(stat='F')
anova_result

(train|>ggplot(aes(x=unemployment_rate,fill=target)) +geom_density(alpha=0.3) ) /(
train|>ggplot(aes(x=unemployment_rate,fill=target)) + geom_boxplot())

```
### inflation_rate
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

anova_result <- 
  train|>
  specify(inflation_rate~target) |>
  hypothesise(null='independence') |>
  calculate(stat='F')
anova_result

(train|>ggplot(aes(x=inflation_rate,fill=target)) +geom_density(alpha=0.3) ) /(
train|>ggplot(aes(x=inflation_rate,fill=target)) + geom_boxplot())

```
### gdp
```{r, fig.height=3, fig.width=6}
## fig.height=240
## fig.width=320

anova_result <- 
  train|>
  specify(gdp~target) |>
  hypothesise(null='independence') |>
  calculate(stat='F')
anova_result

(train|>ggplot(aes(x=gdp,fill=target)) +geom_density(alpha=0.3) ) /(
train|>ggplot(aes(x=gdp,fill=target)) + geom_boxplot())

```

## Models
### data resample
```{r}
set.seed(1234)
df_split <- initial_split(train, prop = 0.8,strata = target)
train_set <- training(df_split)
test_set <- testing(df_split)
cv_folds <- vfold_cv(train_set,
                     v = 3, 
                     repeats = 1,
                     strata = target)
```


### engines
```{r}
glm_eng <- 
  multinom_reg(penalty = 0.01623777,
               mixture = 0.05) |>  # Example penalty and mixture values
  set_engine('nnet') |>
  set_mode("classification")    # Specify classification

lgbm_eng<-
   parsnip::boost_tree(
      trees = 500, # Number of trees
      learn_rate = 0.01,
      tree_depth =5,
      loss_reduction = 0.001,
      stop_iter = 50,
      sample_size = 0.9, # Added sample_size
      #tree_depth = tune(),
      #mtry = 0.5,
      min_n = 100
   ) |>
   set_mode("classification")|>
   set_engine("lightgbm",
              #metric='roc_auc', 
              num_leaves = 20,
              counts = FALSE,
              num_threads=12,
              metric = "auc",              # 优化目标
              # reg_alpha=0.01,
              # reg_lambda = 0.5,
              verbose=1) 

rf_eng<- rand_forest( trees = 700, 
                      #mtry=100, 
                      min_n=100) |>
  set_engine("ranger",num.threads=4)|>
  set_mode("classification") 

xgb_eng<- parsnip::boost_tree( trees = 500, 
                      learn_rate = 0.01,
                      loss_reduction = 0.001,
                      sample_size = 0.8, # Added sample_size
                      #mtry=tune(),
                      min_n=70) |>
  set_engine("xgboost",num.threads=8)|>
  set_mode("classification")
#[1] "use_C5.0"             "use_cubist"           "use_earth"            "use_glmnet"           "use_kernlab_svm_poly" "use_kernlab_svm_rbf" 
#[7] "use_kknn"             "use_ranger"           "use_xgboost" 

c50_eng <- boost_tree() |>
  set_mode('classification')|>
  set_engine('C5.0')

earth_eng <-  # good model base score 0.8718
  mars() %>% 
  set_mode("classification") %>% 
  set_engine("earth") 

svm_eng <- 
  svm_rbf(
    cost = 0.001, 
    rbf_sigma = 0.01
    ) %>% 
  set_mode("classification") 

kknn_eng <- 
  nearest_neighbor(neighbors = 5, 
                   #weight_func = tune()
                   ) %>% 
  set_mode("classification") %>% 
  set_engine("kknn") 

selected_eng <- list(glm=glm_eng,
                     rf=rf_eng,
                     lgbm=lgbm_eng,
                     xgb=xgb_eng,
                     #c50=c50_eng,
                     earth= earth_eng,
                     #kknn=kknn_eng,
                     svm=svm_eng
                     
                     )


```

#### set metrics
```{r}
acu_metrics <- metric_set(accuracy,precision) # main goal is roc_auc, accuracy is just for reference
```


### recipies
```{r}
check_metric <- function(rcp_script,default_engineer='glm'){
  glm_eng <- 
  multinom_reg(penalty = 0.01623777,
               mixture = 0.05) |>  # Example penalty and mixture values
  set_engine('nnet') |>
  set_mode("classification")  
  
  lgbm_eng<-
   parsnip::boost_tree(
      trees = 100, # Number of trees
      learn_rate = 0.1,
      loss_reduction = 0.001,
      stop_iter = 50,
      min_n = 100
   ) |>
   set_mode("classification")|>
   set_engine("lightgbm",
              num_threads=12,
              metric = "multiclass",              # 优化目标
              verbose=1) 
  wfit <- workflow()|>
    add_recipe(rcp_script)
    
  if (default_engineer =='glm'){
    wfit <- 
      wfit |> add_model(glm_eng)
  } else{
    wfit <- 
      wfit |> add_model(lgbm_eng)
  }
    wfit <- 
      wfit |> last_fit(df_split)
  
  wfit|>collect_metrics() |> print()
  return(wfit)
}
```

#### v0
```{r}
rcp_v0 <- 
  recipe(target ~ ., data = train_set) |>
  update_role(id, new_role = 'ID') #|>
  #step_impute_median(all_numeric_predictors()) |>
  #step_normalize(all_numeric_predictors())
```
#### v1_baseline
```{r}
rcp_v1_bs <- 
  recipe(target ~ ., data = train_set) |>
  update_role(id, new_role = 'ID') |>
  step_impute_median(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors())
```

#### v2_factor
```{r}
rcp_v2_fc <- 
  recipe(target ~ ., data = train_set) |>
  update_role(id, new_role = 'ID') |>
  step_impute_median(all_numeric_predictors()) |>
  step_mutate_at(c('marital_status','application_mode','application_order','course'),
                 fn=as_factor)|>
  step_novel(all_nominal_predictors())|>
  step_unknown(all_nominal_predictors())|>
  step_other(all_nominal_predictors())|>
  step_dummy(all_nominal_predictors() )|>
  #step_zv(all_numeric_predictors())|>
  #step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())

rcp_v2_fc |>prep()|>juice()|>glimpse()
```

#### v3_fct_num
```{r}
num_cols <- c('previous_qualification_grade','admission_grade','age_at_enrollment',
              'curricular_units_1st_sem_grade','curricular_units_2nd_sem_grade',
              'unemployment_rate','inflation_rate','gdp')

fct_cols_1 <- c('marital_status','application_mode','application_order','course','daytime_evening_attendance',
              'previous_qualification','nacionality','mothers_qualification',
              'fathers_qualification', 'mothers_occupation', 'fathers_occupation', 
               'displaced', 'educational_special_needs', 
              'debtor', 'tuition_fees_up_to_date', 'gender', 
              'scholarship_holder', 'international')

fct_cols_2 <- c("curricular_units_1st_sem_credited", "curricular_units_1st_sem_enrolled",
              "curricular_units_1st_sem_evaluations", "curricular_units_1st_sem_approved",
              "curricular_units_1st_sem_without_evaluations",
              "curricular_units_2nd_sem_credited", "curricular_units_2nd_sem_enrolled", 
              "curricular_units_2nd_sem_evaluations", "curricular_units_2nd_sem_approved",
               "curricular_units_2nd_sem_without_evaluations"
              )
rcp_v3_num <-
  recipe(target ~ ., data = train_set) |>
  update_role(id, new_role = 'ID') |>
  step_mutate_at(all_of(c(fct_cols_1,fct_cols_2)),fn=as_factor)|>
  step_impute_median(all_numeric_predictors()) |>
  step_log(age_at_enrollment, offset=1) |>
  step_novel(all_nominal_predictors())|>
  step_unknown(all_nominal_predictors())|>
  step_other(all_nominal_predictors())|>
  step_dummy(all_nominal_predictors() )|>
  step_zv(all_numeric_predictors())|>
  step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())

tmp_wfit <- check_metric(rcp_v3_num)
```




#### select_recipes
```{r}
rcp_selected <- list(
  #v1 =rcp_v1_bs,
  v2= rcp_v2_fc,
  v3= rcp_v3_num,
  v0 = rcp_v0
)

```


### workflow

#### simple start

```{r}
set.seed(1234)

simple_wfs_fit_results <- 
  rcp_selected|>
  map_dfr(\(rcp_item) 
          workflow() |> 
            add_recipe(rcp_item) |>
            add_model(glm_eng)|>
            last_fit(df_split)|>
            collect_metrics(),
          .id='source')
simple_wfs_fit_results |>filter(.metric=='accuracy')

# simple_wf_result <-
#   simple_wf_fit|>
#   fit_resamples(cv_folds,
#   #  last_fit(df_split,
#           control = control_resamples(verbose=TRUE),
#            metrics=acu_metrics)
# #plan(sequential)
 #simple_wf_fit |> collect_metrics()
 #   extract_fit_engine()|>
#   plot()
#simple_wf_fit < -simple_wf_fit |> last_fit(df_split) 
```

#### workflowset with multiple reciepes

### Tune hyperparameters

### Stack


```{r}
set.seed(1234)
library(future)
#plan(multisession,workers = 4)
combined_fit <-
  stacks::stacks()|>
  stacks::add_candidates(wfs_result)|>
  stacks::blend_predictions(penalty = 10^seq(-2, -0.1, length = 20))|>
  stacks::fit_members()

combined_fit|>
  autoplot(type = "weights")

autoplot(combined_fit)
#plan(sequential)
```

## Evaluation on Test 

```{r}
final_model <- 
  #combined_fit # stack case - complex 
  simple_wf_fit|>extract_workflow() # simple workflow

combined_test_result <- 
  test_set %>%
  bind_cols(predict(final_model, 
                    new_data=test_set,type='class'))
combined_test_result|>acu_metrics(truth=target, estimate=.pred_class)
```


### Predic & Sumbit

```{r}
set.seed(1234)
library(future)
plan(multisession,workers = 4)

#final_model <- simple_wf_fit|>extract_workflow()
final_predictions <- final_model |>
   predict(new_data = test,type='class')
plan(sequential)

 # #Handle negative predictions
 # final_predictions <- final_predictions |>
 #   mutate(.pred= ifelse(.pred< 0, 0, .pred))

 # Save submission file
 submission |>
   mutate(Target=final_predictions$.pred_class)|>
   readr::write_csv("submission.csv")
 zip('submission.csv.zip','submission.csv')
```

## Submit to kaggle
```{r}
# submit latest submission.csv
system('kaggle competitions submit -c playground-series-s4e6 -f submission.csv.zip -m "inital"')

Sys.sleep(15)
# get latest score 
system('kaggle competitions submissions -q -c playground-series-s4e6')
# 
# # get leader board score
# #system('kaggle competitions leaderboard -s -v -c playground-series-s4e8')
```

### notebook convert
```{r}
 library(rmd2jupyter)
 rmd2jupyter('s4e6_academic_success.Rmd')
```
